{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ***Applying Transfer learning and using the embedded vectors from Doc2Vec to train a Pytorch LSTM Neural Network :***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import newaxis as na"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we got our embedded vectors from all the reviews using doc2vec and saved them in a csv file, we first start by importing the embeddings and then we will split these vectors into train set , validation set and test set and then train different NN and pick up the one that performs the best on the validation set "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ***Getting the arrays of the reviews and the target sentiments :***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data_new=pd.read_csv('./data/doc2vec_dataset.csv',sep='\\t')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def str_to_float(row):\n",
    "    rev=row['embedded review']\n",
    "    rev=re.sub('[\\n]', '', rev).strip('[]').split()\n",
    "    return(np.array(rev,dtype=str).astype(np.float))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data_new['array']=data_new.progress_apply(str_to_float,axis=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 50000/50000 [00:06<00:00, 7314.23it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "X=np.array(data_new['array'].to_list(),dtype='float64')\n",
    "Y=data_new['sentiment'].to_numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ***Splitting the data and Standardizing using X_train :***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X_train,X_val,Y_train,Y_val=train_test_split(X,Y,test_size=0.33,random_state=40)\n",
    "X_test,X_val,Y_test,Y_val=train_test_split(X_val,Y_val,test_size=0.5,random_state=40)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "std=StandardScaler().fit(X=X_train)\n",
    "std"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "X_train_std,X_val_std,X_test_std=std.transform(X_train),std.transform(X_val),std.transform(X_test)\n",
    "print(f'mean of training data after standardizing : {X_train_std.mean():.5f}')\n",
    "print('-'*50)\n",
    "print(f'mean of testing data after standardizing : {X_test_std.mean():.5f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean of training data after standardizing : -0.00000\n",
      "--------------------------------------------------\n",
      "mean of testing data after standardizing : -0.00007\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ***Training our LSTM on classifying the embeddings of the reviews :***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "class lstm(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size,num_layers,dropout):\n",
    "            super(lstm, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.num_layers=num_layers\n",
    "            self.dropout=dropout\n",
    "            self.lstm=torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,batch_first=True, dropout=dropout, bidirectional=False)\n",
    "    \n",
    "            self.drop=torch.nn.Dropout(p=0.3)\n",
    "            self.fc = torch.nn.Linear(self.hidden_size, 2)\n",
    "            self.sigmoid = torch.nn.Sigmoid() \n",
    "            \n",
    "        def _init_hidden(self, current_batch_size):\n",
    "            \"\"\"Sets initial hidden and cell states (for LSTM).\"\"\"\n",
    "\n",
    "            h0 = torch.zeros(self.num_layers , current_batch_size, self.hidden_size).cuda()\n",
    "            c0 = torch.zeros(self.num_layers , current_batch_size, self.hidden_size).cuda()\n",
    "            return h0, c0       \n",
    "        def forward(self, x):\n",
    "            # Forward propagate LSTM\n",
    "            h, c = self._init_hidden(current_batch_size=x.size(0))\n",
    "            out, _ = self.lstm(x, (h, c))\n",
    "\n",
    "            # dropout\n",
    "            out = self.drop(out)\n",
    "\n",
    "            # Decode the hidden state of the last time step\n",
    "            out = self.fc(out[:, -1, :])\n",
    "\n",
    "            return self.sigmoid(out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "X_train_std,Y_train=torch.FloatTensor(X_train_std[:,na,:]).cuda(),torch.FloatTensor(Y_train).long().cuda()\n",
    "X_val_std,Y_val=torch.FloatTensor(X_val_std[:,na,:]).cuda(),torch.FloatTensor(Y_val).long().cuda()\n",
    "X_test_std,Y_test=torch.FloatTensor(X_test_std[:,na,:]).cuda(),torch.FloatTensor(Y_test).long().cuda()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "model = lstm(300, 500,2,0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "model.cuda()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "lstm(\n",
       "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=500, out_features=2, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "model.eval()\n",
    "Y_pred = model(X_val_std)\n",
    "before_train = criterion(Y_pred, Y_val)\n",
    "print(f'Evaluation loss before training   :   {before_train.item():.3f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation loss before training   :   0.693\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "model.train()\n",
    "epoch = 10000\n",
    "batch_size=1000\n",
    "for epoch in tqdm(range(epoch)):\n",
    "    loss_list=[]\n",
    "    for i in range(0,len(X_train_std),batch_size):\n",
    "        X_batch=X_train_std[i:i+batch_size] \n",
    "        Y_batch=Y_train[i:i+batch_size]    \n",
    "        optimizer.zero_grad()    \n",
    "        y_pred = model(X_batch)     # Forward pass    \n",
    "        loss=criterion(y_pred, Y_batch)     # Compute Loss\n",
    "        loss_list.append(loss.item())\n",
    "        loss.backward()     # Backward pass\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch%1000==0:\n",
    "        print('-'*50)\n",
    "        print('Epoch {}: train loss: {:.3f}\\n'.format(epoch, np.mean(loss_list)))    \n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 1/3000 [00:00<18:38,  2.68it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------\n",
      "Epoch 0: train loss: 0.691\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 33%|███▎      | 1001/3000 [04:59<10:03,  3.31it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------\n",
      "Epoch 1000: train loss: 0.509\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 67%|██████▋   | 2001/3000 [10:00<05:01,  3.32it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------\n",
      "Epoch 2000: train loss: 0.494\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3000/3000 [15:00<00:00,  3.33it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "model.eval()\n",
    "Y_pred = model(X_val_std)\n",
    "after_train = criterion(Y_pred, Y_val)\n",
    "print(f'Evaluation loss after training   :   {after_train.item():.3f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation loss after training   :   0.514\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "Y_pred = model(X_test_std)\n",
    "targets_pred=torch.argmax(Y_pred,dim=1).cpu().numpy()\n",
    "Y_test=Y_test.cpu().numpy()\n",
    "test_acc=(targets_pred==Y_test).mean()\n",
    "print(f'Final Test Accuracy   :   {test_acc:.3f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(classification_report(Y_test, targets_pred, target_names=['negative','positive']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"flare\",linecolor=\"red\",vmin=50,vmax=1400)\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True sentiment')\n",
    "  plt.xlabel('Predicted sentiment');\n",
    "fig, axes = plt.subplots(nrows = 1, ncols =1, figsize = (10, 7),facecolor='#FAF7CC')\n",
    "cm = confusion_matrix(Y_test, targets_pred)\n",
    "df_cm = pd.DataFrame(cm, index=['negative','positive'], columns=['negative','positive'])\n",
    "show_confusion_matrix(df_cm)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "74ec93bccee811c650cabd8a679f27f24530bd23ec50915e0d76c126ae7ffac5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}